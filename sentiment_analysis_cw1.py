# -*- coding: utf-8 -*-
"""final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m_dXbzxeTEg1A-PaepJDVyD-ru9GrNhv

All imports shall be done here
"""

# This project is done as part of the NLP coursework and is based on the Sentiment Labelled Sentences.
# This dataset was created for the Paper 'From Group to Individual Labels using Deep Features', Kotzias et. al,. KDD 2015
# Creators: Dimitrios Kotzias

# All imports in the project shall be specified here. Begining
#--------------------------------------------------------------

import pandas as pd
import matplotlib as plt
import seaborn as vis
import re
import string
import nltk
import sklearn
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, auc
import time
import os
#--------------------------------------------------------------
# All imports in the project shall be specified here. Ending

"""## Downloads, run if necessary"""

# All Downloads in the project shall be specified here. Begining
#--------------------------------------------------------------

nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt_tab')

#--------------------------------------------------------------
# All Downloads in the project shall be specified here. Ending

"""## From imports used in the project"""

# All From imports in the project shall be specified here. Begining
#--------------------------------------------------------------

from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier


#--------------------------------------------------------------
# All From imports in the project shall be specified here. Ending

"""## Importing datasets"""

from google.colab import drive
drive.mount('/content/drive')

# accessing the dataset file path
amazon_ds_file_path = '/content/drive/MyDrive/nlp_cw1_data/dataset/amazon_cells_labelled.txt'

# verify that file is exists or not
if os.path.exists(amazon_ds_file_path):
    print("Amazon dataset found !!")
else :
    print("Amazon dataset doesn't exist !!")

# accessing the dataset file path
yelp_ds_file_path = '/content/drive/MyDrive/nlp_cw1_data/dataset/yelp_labelled.txt'

# verify that file is exists or not
if os.path.exists(amazon_ds_file_path):
    print("yelp dataset found !!")
else :
    print("yelp dataset doesn't exist !!")

# accessing the dataset file path
imdb_ds_file_path = '/content/drive/MyDrive/nlp_cw1_data/dataset/imdb_labelled.txt'

# verify that file is exists or not
if os.path.exists(amazon_ds_file_path):
    print("imdb dataset found !!")
else :
    print("imdb dataset doesn't exist !!")

# Importing datasets. Begining
#--------------------------------------------------------------

amazon_ds = pd.read_csv(amazon_ds_file_path, delimiter='\\t', usecols=[0, 1], engine = 'python')
imdb_ds = pd.read_csv(imdb_ds_file_path, delimiter='\\t', usecols=[0, 1], engine = 'python')
yelp_ds = pd.read_csv(yelp_ds_file_path, delimiter='\\t', usecols=[0, 1], engine = 'python')

#--------------------------------------------------------------
# Importing datasets. Ending

"""## Setting up the dataset"""

# Setting up the dataset. Begining
#--------------------------------------------------------------

amazon_ds.columns = ['Description', 'Review']
imdb_ds.columns = ['Description', 'Review']
yelp_ds.columns = ['Description', 'Review']

#--------------------------------------------------------------
# Setting up the dataset. Ending

"""## Combining datasets"""

# Combining dataset. Begining
#--------------------------------------------------------------

comb_ds = pd.concat([amazon_ds, imdb_ds, yelp_ds], ignore_index = True)

#print(comb_ds.head(10)) #Uncomment this line to view the first 10 lines of the combined dataset
#print(comb_ds.tail(10)) #Uncomment this line to view the last 10 lines of the combined dataset

#--------------------------------------------------------------
# Combining dataset. Ending

"""## Preprocessing"""

# Preprocessing of dataset. Begining
#--------------------------------------------------------------

comb_ds['Clean_Description'] = comb_ds['Description'].apply(lambda x: re.sub(r'[@#]', '', x)) #removing @ and #
comb_ds['Punct_Removed_Description'] = comb_ds['Clean_Description'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation))) #removing punctuations
comb_ds['Digit_Removed_Description'] = comb_ds['Punct_Removed_Description'].apply(lambda x: re.sub(r'\d+', '', x)) #removing digits
comb_ds['Lowercase_Description'] = comb_ds['Digit_Removed_Description'].apply(lambda x: x.lower()) #converting to lowercase
#removing stopwords
comb_ds['Tokenized_Description'] = comb_ds['Lowercase_Description'].apply(word_tokenize)

stop_words = set(stopwords.words('english'))
comb_ds['Stopwords_Removed_Text'] = comb_ds['Tokenized_Description'].apply(
    lambda tokens: list(filter(lambda word: word not in stop_words, tokens))
)

lemmatizer = WordNetLemmatizer()

comb_ds['Lemmatized_Text'] = comb_ds['Stopwords_Removed_Text'].apply(lambda tokens: list(map(lemmatizer.lemmatize, tokens)))

comb_ds['Processed_Text'] = comb_ds['Lemmatized_Text'].apply(' '.join)

comb_ds.head(5)

#--------------------------------------------------------------
# Preprocessing of dataset. Ending

"""## Splitting to training, validating and testing datasets"""

# Splitting data into train (70%), temp (30%)

X = comb_ds['Processed_Text']   # Feature (preprocessed text)
y = comb_ds['Review']           # Target (sentiment label: 0 or 1)

# First split: Train (70%) & Temp (30%)
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)

# Second split: Temp (30%) - Validation (15%) & Test (15%)
X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Display dataset sizes
print("\n Dataset Sizes \n")
print(f"Train size: {len(X_train)}")
print(f"Validation size: {len(X_valid)}")
print(f"Test size: {len(X_test)}")

#--------------------------------------------------------------
# Splitting to training, validating and testing datasets. Ending

"""## TF-IDF extraction"""

# TF-IDF extraction. Begining
#--------------------------------------------------------------

# Initialize TF-IDF Vectorizer
tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # Limits vocabulary size to 5000 most important words

# Fit and transform the training data
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)

# Transform the validation and test data (using the same vocabulary)
X_valid_tfidf = tfidf_vectorizer.transform(X_valid)
X_test_tfidf = tfidf_vectorizer.transform(X_test)

#--------------------------------------------------------------
# TF-IDF extraction. Ending

"""## Training models"""

## Training models Begining
#--------------------------------------------------------------

## Training and testing different models, here I am taking the following 5 models,
## Logistic Regression (LR) - Best for binary classification (0 or 1 sentiment). Fast, interpretable, and performs well on text data.
print("\n Training Models \n")
lr_model = LogisticRegression()
lr_model.fit(X_train_tfidf, y_train)
y_predict_lr = lr_model.predict(X_test_tfidf)
print("Logistic Regression Accuracy:", accuracy_score(y_test, y_predict_lr))

## Support Vector Machine (SVM) - Uses hyperplanes for high-margin classification. Works well with small to medium datasets.

svm_model = SVC()
svm_model.fit(X_train_tfidf, y_train)
y_predict_svm = svm_model.predict(X_test_tfidf)
print("SVM Accuracy:", accuracy_score(y_test, y_predict_svm))

## Naïve Bayes (MultinomialNB) - Based on probability and Bayes theorem. Works well for text classification, especially spam detection.

nb_model = MultinomialNB()
nb_model.fit(X_train_tfidf, y_train)
y_predict_nb = nb_model.predict(X_test_tfidf)
print("Naïve Bayes Accuracy:", accuracy_score(y_test, y_predict_nb))

## Random Forest (RF) - Uses multiple decision trees to improve accuracy & stability. Less affected by overfitting than single decision trees.

rf_model = RandomForestClassifier()
rf_model.fit(X_train_tfidf, y_train)
y_predict_rf = rf_model.predict(X_test_tfidf)
print("Random Forest Accuracy:", accuracy_score(y_test, y_predict_rf))

## Gradient Boosting (XGBoost, LightGBM, CatBoost) - Advanced decision tree-based models for high performance. Great for competitions, but can be slower than simpler models.

xgb_model = XGBClassifier(eval_metric='logloss')
xgb_model.fit(X_train_tfidf, y_train)
y_predict_xgb = xgb_model.predict(X_test_tfidf)

# Accuracy
print("XGBoost Accuracy:", accuracy_score(y_test, y_predict_xgb))

#--------------------------------------------------------------
## Training and testing models Ending

"""## Comparing models"""

## Comparing models. Begining
#--------------------------------------------------------------

# Store accuracy scores in a dictionary
accuracy_scores = {
    "Logistic Regression": accuracy_score(y_test, y_predict_lr),
    "SVM": accuracy_score(y_test, y_predict_svm),
    "Naïve Bayes": accuracy_score(y_test, y_predict_nb),
    "Random Forest": accuracy_score(y_test, y_predict_rf),
    "XGBoost": accuracy_score(y_test, y_predict_xgb)
}

print("\n Accuracy scores for each model \n")
for model, acc in accuracy_scores.items():
    print(f"{model}: {acc:.4f}")

# Find the best model
best_model = max(accuracy_scores, key=accuracy_scores.get)
print(f"\n Best Performing Model: {best_model} with Accuracy: {accuracy_scores[best_model]:.4f}")

#--------------------------------------------------------------
## Comparing models. Ending

"""## Evaluate each model"""

# Dictionary to store all metrics
model_metrics = {}

# List of models and their predictions
models = {
    "Logistic Regression": (lr_model, y_predict_lr),
    "SVM": (svm_model, y_predict_svm),
    "Naïve Bayes": (nb_model, y_predict_nb),
    "Random Forest": (rf_model, y_predict_rf),
    "XGBoost": (xgb_model, y_predict_xgb)
}

## Evaluate each model
for model_name, (model, y_pred) in models.items():
    start_time = time.time()
    model.fit(X_train_tfidf, y_train)  # Measure training time
    train_time = time.time() - start_time

    start_time = time.time()
    model.predict(X_test_tfidf)  # Measure inference time
    inference_time = time.time() - start_time

    model_metrics[model_name] = {
        "Accuracy": accuracy_score(y_test, y_pred),
        "Precision": precision_score(y_test, y_pred, average='binary'),
        "Recall": recall_score(y_test, y_pred, average='binary'),
        "F1-Score": f1_score(y_test, y_pred, average='binary'),
        "ROC-AUC": roc_auc_score(y_test, y_pred),
        "Confusion Matrix": confusion_matrix(y_test, y_pred),
        "Training Time (s)": train_time,
        "Inference Time (s)": inference_time
    }

# Convert metrics dictionary to a DataFrame
metrics_df = pd.DataFrame(model_metrics).T  # Transpose to make models as rows

# Drop confusion matrix column (since it's not a numeric metric)
metrics_df = metrics_df.drop(columns=["Confusion Matrix"])

"""## Plot bar charts for each metric"""

metrics_df

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Exclude ROC-AUC from plotting
metrics_to_plot = metrics_df.drop(columns=["ROC-AUC"])

# Define unique colors for each model
bar_colors = ["skyblue", "lightcoral", "lightgreen", "gold", "mediumpurple"]

# Define number of subplots
num_metrics = len(metrics_to_plot.columns)
fig, axes = plt.subplots(nrows=(num_metrics // 2) + (num_metrics % 2), ncols=2, figsize=(14, 10))

# Flatten axes array for easier iteration
axes = axes.flatten()

# Plot each metric in a subplot
for i, metric in enumerate(metrics_to_plot.columns):
    bars = axes[i].bar(metrics_to_plot.index, metrics_to_plot[metric], color=bar_colors[:len(metrics_to_plot.index)])  # Assign different colors to bars
    axes[i].set_xlabel("Models")
    axes[i].set_ylabel(metric)
    axes[i].set_title(f"Model Comparison - {metric}")
    axes[i].tick_params(axis='x', rotation=30)
    # axes[i].grid(axis="y")

    # Set y-axis limit to 1 and add more precision
    max_value = metrics_to_plot[metric].max()
    axes[i].set_ylim(0, max(1, max_value + 0.05))  # Extend slightly beyond max value
    axes[i].set_yticks(np.linspace(0, max(1, max_value + 0.05), 10))  # More y-ticks
    axes[i].yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f"{x:.2f}"))  # Two decimal places

    # Move legend outside the plot
    axes[i].legend(bars, metrics_to_plot.index, title="Models", bbox_to_anchor=(1.05, 1), loc="upper left", fontsize=8)

# Adjust layout
plt.tight_layout()
plt.show()

"""## Plot ROC Curve for each model"""

# Plot ROC Curve for each model
plt.figure(figsize=(10, 6))

for model_name, (model, y_pred) in models.items():
    fpr, tpr, _ = roc_curve(y_test, y_pred)
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f"{model_name} (AUC = {roc_auc:.2f})")

plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line for random guessing
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve Comparison")
plt.legend()
plt.grid()
plt.show()

"""## Confusion Matrix as a Heatmap"""

for model, metrics in model_metrics.items():
    conf_matrix = pd.DataFrame(metrics["Confusion Matrix"],
                               index=["Actual Negative", "Actual Positive"],
                               columns=["Predicted Negative", "Predicted Positive"])

    plt.figure(figsize=(5, 4))
    vis.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", linewidths=0.5)
    plt.title(f"Confusion Matrix - {model}")
    plt.xlabel("Predicted Labels")
    plt.ylabel("True Labels")
    plt.show()
    print("\n\n\n")

"""## Model Comparison Summary (Heatmap)
# Convert metrics dictionary to a DataFrame

"""

metrics_df = pd.DataFrame(model_metrics).T

# Drop confusion matrix column (not numeric)
metrics_df = metrics_df.drop(columns=["Confusion Matrix"])

# Convert to float for heatmap visualization
metrics_df = metrics_df.astype(float)

# Plot Heatmap for Model Comparison
plt.figure(figsize=(10, 6))
vis.heatmap(metrics_df, annot=True, cmap="coolwarm", linewidths=0.5, fmt=".3f")
plt.title("Model Performance Comparison")
plt.xlabel("Metrics")
plt.ylabel("Models")
plt.show()

import pandas as pd

# Assuming metrics_df has been calculated
print(metrics_df)

